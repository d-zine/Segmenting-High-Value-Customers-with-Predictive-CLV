{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba505391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|███████████████████████████████████████████████████████████████████| 13/13 [05:59<00:00, 27.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 MAE Train MAE Test\n",
      "Model                              \n",
      "MLR                 $39.11   $40.65\n",
      "Ridge               $39.08   $40.65\n",
      "Lasso               $39.89   $41.37\n",
      "Elastic Net         $42.08   $43.57\n",
      "Decision Tree       $54.45   $53.03\n",
      "Random Forest       $38.77   $39.51\n",
      "XGBoost             $39.31   $40.48\n",
      "SVR                 $45.03   $46.28\n",
      "Stacked Ensemble    $38.12   $39.03\n",
      "LGBM                $36.30   $37.02\n",
      "MLP                 $37.44   $40.74\n",
      "Kernel Ridge        $39.48   $40.98\n",
      "KNN                 $40.27   $42.06\n",
      "\n",
      "Best Model (based on MAE): LGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "imputed_data = pd.read_csv(\"sbux_clv_drop_AOV_and_completed_offers_across_channels_with_dummies.csv\")\n",
    "\n",
    "#After calling get dummies in preprocessing phase, \n",
    "#Drop Gender_Unknown, Age_Unknown, HH Income_Unknown to reduce multicollinearity\n",
    "# Prepare the data\n",
    "X = imputed_data[['MemberSince',\n",
    "       'Recency (# Days ago from last trans. As of final day up to Day 15)',\n",
    "       'Frequency (# Trans. from Day 1-15)',\n",
    "       'Monetary Value (Sum of Trans. from Day 1-15)',\n",
    "       '# of Marketing offers that Starbucks sent to each customer from Day 1-15',\n",
    "       '# of Marketing offers that were viewed from Day 1-15',\n",
    "       '# of Marketing offers that were successfully completed from Day 1-15',\n",
    "       'Marketing Offer View Rate from Day 1-15',\n",
    "       'Marketing Offer Response Rate from Day 1-15',\n",
    "       'Age_18-34', 'Age_35-50', 'Age_51-67',\n",
    "       'Age_68-84', 'Age_85-101', 'Gender_F', 'Gender_M',#'Age_Unknown',\n",
    "       'Gender_O', 'HH Income_100k-120k',#'Gender_Unknown','HH Income_Unknown'\n",
    "       'HH Income_30k-50k', 'HH Income_50k-75k', 'HH Income_75k-100k']]\n",
    "   \n",
    "\n",
    "y = imputed_data['Sum(Trans. Amt from Day 16-30)']\n",
    "\n",
    "# Define the models\n",
    "mlr = LinearRegression()\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "elastic = ElasticNet()\n",
    "dt = DecisionTreeRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "xgb = XGBRegressor()\n",
    "svr = SVR()\n",
    "lgbm = LGBMRegressor()\n",
    "mlp = MLPRegressor()\n",
    "kr = KernelRidge()\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Define base estimators to be used in the ensemble\n",
    "base_estimators = [('rf', RandomForestRegressor()),\n",
    "                   ('xgb', XGBRegressor()),\n",
    "                   ('dt', DecisionTreeRegressor())]\n",
    "\n",
    "# Define a final estimator to combine the predictions of the base estimators\n",
    "final_estimator = StackingRegressor(estimators=[('ridge', RidgeCV()),\n",
    "                                                 ('lasso', LassoCV())])\n",
    "\n",
    "# Define a StackingRegressor object with the base and final estimator\n",
    "stacked = StackingRegressor(estimators=base_estimators, final_estimator=final_estimator)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Fit and evaluate each model using cross-validation\n",
    "models = {'MLR': mlr, 'Ridge': ridge, 'Lasso': lasso, 'Elastic Net': elastic, 'Decision Tree': dt, 'Random Forest': rf, 'XGBoost': xgb, 'SVR': svr, 'Stacked Ensemble': stacked, 'LGBM':lgbm, 'MLP':mlp, 'Kernel Ridge': kr, 'KNN': knn}\n",
    "\n",
    "# Initialize variables to track the best model and its MAE\n",
    "best_model_mae = None\n",
    "best_mae = np.inf\n",
    "\n",
    "metrics_dict = {}\n",
    "for name, model in tqdm(models.items(), desc=\"Processing models\"):\n",
    "    metrics_dict[name] = []\n",
    "\n",
    "    # Evaluate the model using cross-validation on the training set\n",
    "    mae_train = -cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "    metrics_dict[name].extend([mae_train.mean(), mae_test])\n",
    "\n",
    "    # Check if current model has lower MAE\n",
    "    if mae_test < best_mae:\n",
    "        best_model_mae = name\n",
    "        best_mae = mae_test\n",
    "\n",
    "# Create a DataFrame and print the results\n",
    "metrics_df = pd.DataFrame(metrics_dict, index=['MAE Train', 'MAE Test']).T\n",
    "metrics_df['MAE Train'] = metrics_df['MAE Train'].apply(lambda x: f'${x:.2f}')\n",
    "metrics_df['MAE Test'] = metrics_df['MAE Test'].apply(lambda x: f'${x:.2f}')\n",
    "metrics_df.index.name = 'Model'\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "# Print the best model for MAE\n",
    "print(f\"\\nBest Model (based on MAE): {best_model_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a6c66",
   "metadata": {},
   "source": [
    "# My Winner Model is LGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8760f",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
